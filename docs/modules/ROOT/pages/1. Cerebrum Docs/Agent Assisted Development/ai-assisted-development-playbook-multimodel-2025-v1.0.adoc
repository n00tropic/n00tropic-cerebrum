= *AI-Assisted Development Playbook (Multi-Model, 2025)*
:page-tags: diataxis:reference, domain:platform, audience:contrib, stability:beta
:reviewed: 2025-11-11

This is a consolidated spec merging the core end-to-end (E2E) workflow with
Pro-tier optimizations. It's designed for projects in Python, TypeScript, and
other popular languages and tooling (for example, React, Node.js, Django, AWS). The workflow
leverages each AI's strengths, assuming Pro tiers for unlimited access, advanced
features, and higher limits. This hybrid approach boosts productivity by 2-3x,
with cleaner code and fewer errors.

== AI Strengths

* *Grok (xAI)*: High-level reasoning, large context (up to 512k tokens in Grok 4
 for mega repositories), real-time tooling (web/X search, code sandbox), and fast
 processing (Grok Code Fast 1 at 92 tokens/sec). Ideal for planning,
 architecture, debugging, and STEM tasks. Pro perks: Grok 4 access, "Heavy" mode
 for parallel reasoning on complex refactors.
 https://docs.x.ai/docs/models/grok-4[9]
* *GitHub Copilot (Microsoft)*: Real-time IDE integration, autocomplete, and
 pattern recognition (30-50% productivity boost). Best for in-flow coding, fixes,
 and team workflows like PR reviews. Pro perks: Copilot Workspace for
 automating features/PRs from issues.
// vale Microsoft.Terms = NO
 https://docs.github.com/en/copilot/using-github-copilot/coding-agent/asking-copilot-to-create-a-pull-request[10]
// vale Microsoft.Terms = YES
* *OpenAI Codex*: Natural language-to-code translation, refactoring, and bug
 fixing (75%+ accuracy). Strong for scaffolding and abstract strategies. Pro
 perks: Higher API quotas (for example, GPT-4o fine-tunes) for batch tasks.
* *Google Gemini (2.5 Pro / 2.5 Flash / Flash‑Lite)*: Long-context endpoints
 (up to ~1M tokens on supported endpoints) and strong multimodal reasoning;
 excellent latency/throughput with Flash/Flash‑Lite; tight Vertex AI integration
 for governance and grounding.
 https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models[1]
* *Anthropic Claude (Opus / latest Sonnet)*: Reliable high‑depth reasoning, large
 context, and robust *computer‑use* automation for desktop/browser workflows. Strong
 at repository-wide analysis and test generation.
 https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/computer-use-tool[3]
* *Meta Llama 3.x (open‑weights)*: Open, self‑hostable models with ~128K context
 support (variant‑dependent); good coding ability and privacy‑preserving on‑prem
 deployments.
 https://aws.amazon.com/blogs/aws/introducing-llama-3-2-models-from-meta-in-amazon-bedrock-a-new-generation-of-multimodal-vision-and-lightweight-models/[4]
* *Mistral Codestral (25.xx)*: Code‑specialised model optimised for
 *fill‑in‑the‑middle (FIM)*, low‑latency in‑editor completions, and surgical
 fixes. https://docs.mistral.ai/models/codestral-25-01[5]
* *Cohere Command A / Command A Vision*: Enterprise‑grade agents/RAG and long
 context (Command A up to ~256K); Vision variant supports text+images with 128K
 context for doc/chart OCR. https://docs.cohere.com/docs/command-a-vision[6]
* *DeepSeek Coder V2 (open)*: Open Mixture‑of‑Experts code models with 128K
 context; strong code generation/repair; useful for local, cost‑efficient batch
 refactors. https://github.com/deepseek-ai/DeepSeek-Coder-V2[8]
Free tiers work for light use, but Pro enables seamless scaling (for example, no query
limits, Codex API batches at $0.02/1k tokens).

== Implementation Scripts

This playbook has been implemented as executable scripts in the workspace
automation directory. Use these scripts to run the workflow phases:

* *Planning & Research*:
 `.dev/automation/scripts/ai-workflows/planning-research.sh`
* *Architecture & Design*:
 `.dev/automation/scripts/ai-workflows/architecture-design.sh`
* *Core Coding*: `.dev/automation/scripts/ai-workflows/core-coding.sh`
* *Debugging & Testing*:
 `.dev/automation/scripts/ai-workflows/debugging-testing.sh`
* *Review & Deployment*:
 `.dev/automation/scripts/ai-workflows/review-deployment.sh`
* *Workflow Runner*: `.dev/automation/scripts/ai-workflows/ai-workflow-runner.sh`
 (orchestrates all phases)
Run `./ai-workflow-runner.sh` to access an interactive menu for running
individual phases or the complete workflow. All scripts generate artifacts in
`artifacts/ai-workflows/[phase]/` and log capability runs.

== Workflow Phases

Total cycle time: 20-50% faster than solo coding. Use VS Code extensions to
query all AIs from one IDE (for example, Grok Code Fast in Copilot preview for free
speed boosts). Pipe outputs via copy-paste or scripts.

=== 1. Planning & Research (Led by Grok: 10-20% of workflow)

* *Why Grok?* Handles ambiguity, research, and synthesis best.
* *Steps:* ** Describe project (for example, "Plan a TypeScript React app with Python
 FastAPI back end for auth and real-time chat"). ** Grok researches
 dependencies/trends (for example, web/X search for "2025 TypeScript auth libraries").
 ** Output: High-level spec, stack recommendations, pitfalls.
* *Pro Enhancements:* Use Grok 4's expanded context for detailed plans; enable
 "Heavy" mode for multi-faceted analysis.
* *Involve Others:* Query Codex for pseudocode sketches if needed.
* *Fit for Python/TS:* Suggests async patterns or type safety.
* *Transition:* Export as Markdown; paste into IDE.
=== 2. Architecture & Design (Led by Grok + Codex: 15-25%)

* *Why this split?* Grok for diagrams/reasoning; Codex for code skeletons.
* *Steps:* ** Grok generates architecture (for example, "Draw Mermaid diagram for
 microservices in Python/TS"). ** Feed to Codex (API/Playground): "Generate
 TypeScript interfaces and Python models from spec." ** Iterate: Paste back to
 Grok for validation.
* *Pro Enhancements:* Grok 4 for 512k repository-wide designs; Codex fine-tunes for
 custom patterns.
* *Fit for Python/TS:* Codex for TS types; Grok for cross-lang integration.
* *Output:* Diagrams, stub files. Use Grok's sandbox for early testing.
=== 3. Core Coding & Implementation (Led by Copilot: 30-40%)

* *Why Copilot?* Seamless IDE assistance without switching.
* *Steps:* ** Import stubs into VS Code/JetBrains with Copilot. ** Autocomplete
 functions/imports (for example, Python pandas; TS React hooks). ** Use Copilot Chat:
 "@copilot implement auth middleware in TypeScript." ** For blocks, query Codex:
 "Refactor Python class for concurrency."
* *Pro Enhancements:* Copilot Workspace automatically generates features from issues (for example,
 "@workspace build TS auth flow").
* *Involve Grok:* For hallucinations, paste snippets for second opinions.
* *Fit for Python/TS:* Adapts to codebase patterns.
* *Output:* Functional code with automated tests/comments.
=== 4. Debugging, Testing & Optimization (Led by Grok + Codex: 15-25%)

* *Why this split?* Grok for repository-wide debugging; Codex for strategic fixes.
* *Steps:* ** Paste errors to Grok: "Debug TS loop; run in sandbox." ** Grok
 analyzes/executes tests (for example, Python units). ** Use Codex: "Fix SQL injection
 in Python; propose diff." ** Copilot for inline patches.
* *Pro Enhancements:* Grok 4's large context for full-repository debugs; Codex
 batch-refactors with custom instructions (for example, "Optimize Python for AWS
 Lambda").
* *Fit for Python/TS:* Grok for math bugs; Codex for back end logic.
* *Output:* Bug-free, optimized code (for example, 20% faster Python runtime).
=== 5. Review, Deployment & Iteration (Led by Copilot: 5-10%)

* *Why Copilot?* Integrated with GitHub for PRs/ci.
* *Steps:* ** Push code; Copilot automatically creates PRs/reviews. ** For issues (for example,
 TS builds), query Grok/Codex. ** Post-deploy: Grok monitors X/user feedback.
* *Pro Enhancements:* Copilot Workspace for end-to-end automated PRs; Grok 4 for
 real-time trend analysis.
* *Fit for Python/TS:* Handles ci YAML; researches cloud tooling.
== Phase Summary Table

|=== | Phase | Primary AI | Backup AIs | Time Savings | Example in Python/TS
Project | Pro Enhancements

| Planning/Research | Grok; Gemini
https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models[1]; Claude
https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/computer-use-tool[3]
| Command A
// vale Google.We = NO
// vale Microsoft.We = NO
https://docs.oracle.com/en-us/iaas/Content/generative-ai/cohere-command-a-03-2025.htm[6];
// vale Google.We = YES
// vale Microsoft.We = YES
Llama 3.x (private)
https://aws.amazon.com/blogs/aws/introducing-llama-3-2-models-from-meta-in-amazon-bedrock-a-new-generation-of-multimodal-vision-and-lightweight-models/[4]
| 40% | Research TS auth libs; spec Python back end. | Grok 4 expanded context;
Vertex grounding; Claude computer‑use.

| Architecture/Design | Grok; Claude
https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/computer-use-tool[3];
Gemini https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models[1] |
Llama 3.x
https://aws.amazon.com/blogs/aws/introducing-llama-3-2-models-from-meta-in-amazon-bedrock-a-new-generation-of-multimodal-vision-and-lightweight-models/[4];
 Codex; Command A
// vale Google.We = NO
// vale Microsoft.We = NO
https://docs.oracle.com/en-us/iaas/Content/generative-ai/cohere-command-a-03-2025.htm[6]
// vale Google.We = YES
// vale Microsoft.We = YES
| 30% | Diagram TS components; generate Python models. | Grok 4 for
mega‑designs; Vertex model garden; Claude acceptance criteria.

| Coding | Copilot
// vale Microsoft.Terms = NO
https://docs.github.com/en/copilot/using-github-copilot/coding-agent/asking-copilot-to-create-a-pull-request[10];
// vale Microsoft.Terms = YES
Codestral https://docs.mistral.ai/models/codestral-25-01[5]; DeepSeek Coder V2
https://github.com/deepseek-ai/DeepSeek-Coder-V2[8]; Llama 3.x (local)
https://aws.amazon.com/blogs/aws/introducing-llama-3-2-models-from-meta-in-amazon-bedrock-a-new-generation-of-multimodal-vision-and-lightweight-models/[4]
| Grok (validation) | 50% | Autocomplete TS hooks; suggest Python async. |
Copilot Workspace automated feature generation; FIM completions; 128K local contexts.

| Debugging/Testing | Grok; Claude
https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/computer-use-tool[3];
Gemini https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models[1] |
DeepSeek Coder V2 https://github.com/deepseek-ai/DeepSeek-Coder-V2[8]; Codestral
https://docs.mistral.ai/models/codestral-25-01[5] | 35% | Sandbox Python tests;
refactor TS bugs. | Grok 4 full‑repository debugs; Claude computer‑use; Gemini log
summarisation.

| Review/Deployment | Copilot
// vale Microsoft.Terms = NO
https://docs.github.com/en/copilot/using-github-copilot/coding-agent/asking-copilot-to-create-a-pull-request[10];
// vale Microsoft.Terms = YES
Command A
// vale Google.We = NO
// vale Microsoft.We = NO
https://docs.oracle.com/en-us/iaas/Content/generative-ai/cohere-command-a-03-2025.htm[6];
// vale Google.We = YES
// vale Microsoft.We = YES
Gemini https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models[1] |
Grok; Llama 3.x
https://aws.amazon.com/blogs/aws/introducing-llama-3-2-models-from-meta-in-amazon-bedrock-a-new-generation-of-multimodal-vision-and-lightweight-models/[4]
| 25% | Auto‑PR TS changes; deploy Python app. | PRs from issues; policy‑aware
summaries; Vertex evals/quotas. |===

== Overall Tips

* *Integration:* Query all from IDE; use scripts for piping.
* *Cost/Scaling:* Pro tiers minimize interruptions (for example, unlimited Grok 4
 queries, Codex API batches at $0.02/1k tokens).
* *Switching:* Rotate for stuck tasks--Copilot for speed, Grok for depth, Codex
 for precision.
* *Metrics:* 2-3x output boost, 75%+ accuracy, reduced errors.

== Appended: Additional Models That Plug In Cleanly

This section adds high‑value models not covered earlier and shows where they slot
into the same five‑phase workflow. Keep your stack modular: mix provider
strengths per phase and keep a local/open path ready for privacy‑sensitive code.

=== Google Gemini (2.5 Pro / 2.5 Flash / 2.5 Flash‑Lite)

* *Strengths:* Long context (up to ~1M tokens depending on endpoint), strong
 multimodal reasoning, low-latency *Flash/Flash‑Lite* support, and tight *Vertex AI*
 integration for enterprise deployment and governance.
* *Where it shines in the phases:* ** *1 · Planning/Research:* Use Gemini Pro/2.5
 Pro for broad synthesis and fast literature triage; enable grounding to internal
 docs via Vertex AI Search. ** *2 · Architecture/Design:* Feed large specs/design
 RFCs as single contexts; ask for Mermaid diagrams and interface contracts. ** *3
 · Coding:* Use Flash/Flash‑Lite for chatty, in‑flow code iteration; generate
 scaffolds and quick utilities. ** *4 · Debug/Test:* Summarise long logs; reason
 over failing test shards; propose minimal patches. ** *5 · Review/Deploy:* Good
 fit if you already deploy on GCP--use Vertex AI evals, tracing, and quota
 controls.
* *Notes:* Prefer *Pro* for deeper reasoning; pick *Flash/Flash‑Lite* when
 latency/throughput dominates.
=== Anthropic Claude (Opus • Sonnet 3.5/4.x)

* *Strengths:* Strong reasoning and coding, reliable tool/computer‑use automation,
 large context (hundreds of k tokens), available via *Anthropic API*, *AWS
 Bedrock*, and *Vertex AI*.
* *Where it shines:* ** *1:* De‑risk plans by having Claude enumerate unknowns and
 propose experiments. ** *2:* Validate architecture for invariants and threat
 models; generate acceptance criteria. ** *3:* High‑quality refactors; translate
 code between languages; generate tests that actually run. ** *4:* Excellent at
 repository-wide debugging and systematic failure analysis; pairs well with ci logs. **
 *5:* Draft precise PR reviews; good at change‑risk notes and rollback plans.
* *Notes:* Use *Opus* (or latest Sonnet w/ "`computer use`") for heavy reasoning
 and interactive tool control.
=== Meta Llama 3.x (open‑weights family)

* *Strengths:* Open models (for example, Llama 3.1/3.2) with strong coding ability, long
 contexts (up to ~128k+ depending on variant), and on‑prem/air‑gapped
 deployability.
* *Where it shines:* ** *1--2:* Private reading of design docs/RFCs that can't
 leave your network. ** *3:* Local code completion/autofix where IP or compliance
 prohibits cloud providers. ** *4--5:* On-prem automation for log triage and internal
 PR review.
* *Notes:* Pair with retrieval (FAISS/pgvector or corporate search) and a
 lightweight guardrail (for example, Llama Guard) for production.
=== Mistral Codestral (latest 25.08 series)

* *Strengths:* Code‑specialised models optimised for *fill‑in‑the‑middle (FIM)*
 and low‑latency completions; competitive quality with efficient serving.
* *Where it shines:* ** *3:* In‑editor completions and small function synthesis.
 ** *4:* Narrow, surgical bug‑fix proposals where speed matters more than long
 deliberation.
* *Notes:* Great drop‑in for local/on‑prem coding assistance; keep prompts short
 to exploit FIM.
=== Cohere Command A (and Command A Vision)

* *Strengths:* Enterprise‑grade RAG, tool use, and long context
 (order‑of‑hundreds‑of‑k tokens depending on endpoint). Efficient throughput and
 a strong retrieval stack (Embed/Rerank families).
* *Where it shines:* ** *1:* Evidence-gated research with citations from your
 corpora. ** *2:* Generate/open-refine API contracts from domain docs. ** *3--4:*
 Structured automation that calls tooling (issue trackers, build systems) and writes
 tests. ** *5:* Policy-constrained PR summaries and release notes.
* *Notes:* Good vendor if you need on‑prem or strict data‑isolation options.
=== DeepSeek Coder V2 (open)

* *Strengths:* Open MoE code models with long context (up to ~128k), strong
 completion and repair, and permissive self‑hosting.
* *Where it shines:* ** *3:* Repository-aware completions and migration scripts. ** *4:*
 Batch refactors and static‑analysis‑guided fixes.
* *Notes:* Useful as a cost‑efficient local assistant; wrap with tests and
 linters.

=== Phase Plug‑In Matrix (new models)

|===
| Model | 1 Planning/Research | 2 Architecture/Design | 3 Coding | 4 Debug/Test | 5 Review/Deploy

| *Gemini 2.5 Pro/Flash*
| Synthesis; trend scan; Vertex grounding
| Diagram/spec digestion
| Fast scaffolds & utilities
| Log/test triage
| GCP/Vertex integration

| *Claude (Opus/Sonnet)*
| Risk analysis; experiments
| Arch reviews; threat models
| Refactors; test gen
| Repository-wide debugging
| PR reviews & risk notes

| *Llama 3.x (open)*
| Private doc reading
| Private design reviews
| Local completion
| On-prem log automation
| Internal PR automation

| *Codestral (25.08)*
| --
| --
| FIM completions; helpers
| Quick bug‑fix props
| --

| *Cohere Command A*
| Evidence-gated RAG
| Contract/type synthesis
| Tool-enabled automation
| Test writing + RAG
| Policy-aware summaries

| *DeepSeek Coder V2*
| --
| --
| Repository migration scripts
| Batch refactors
| --
|===

TIP: keep each model behind a tiny interface ("planner," "coder," "debugger,"
"reviewer"). Swap providers without touching calling code.

=== Minimal Integration Recipes

* *Vertex AI (Gemini):* Use a single gateway for auth, quotas, evals, and
 grounding; start with Pro for depth, fall back to Flash/Flash‑Lite in hot paths.
* *Anthropic (Claude):* Enable tool/computer‑use to let Claude drive browsers/IDEs
 for repetitive web or console tasks.
* *Open‑weights (Llama / DeepSeek / Codestral):* Serve via TGI/vLLM on‑prem; add
 RAG + guardrails; log prompts/outputs to your existing observability stack.
* *Cohere:* Combine Command A with Embed/Rerank for end‑to‑end RAG that respects
 enterprise search and access controls.
=== Calibration & Caveats

* Specs (contexts, pricing, and features) change frequently. Keep a small "`model
 registry`" in your repository with live checks and smoke tests per provider.
* Avoid hard‑coding vendors in business logic; dependency‑inject a *ModelRole* to
 keep swaps cheap.
=== References
